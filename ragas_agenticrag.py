# -*- coding: utf-8 -*-
"""RAGAS-AgenticRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10C-5LY6xxzzVn8q5H-aVcJt0aP7RrL1T
"""

# =========================== INSTALL ===========================
!pip install -q langchain sentence-transformers groq langchain-groq faiss-cpu pypdf openai langchain-community langgraph langchain-openai ragas datasets pandas numpy langchain-huggingface
!pip install -q --upgrade ragas

# =========================== IMPORTS ===========================
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_groq import ChatGroq
from langchain_core.runnables import RunnableLambda, RunnableMap, RunnablePassthrough
from langgraph.graph import StateGraph, END
from IPython.display import display, Markdown
import networkx as nx
from langchain_openai import ChatOpenAI

# =========================== STATE ===========================
from typing import TypedDict
from google.colab import userdata
os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY_')
class SetupChainState(TypedDict):
    pdf_path: str
    loader: object
    documents: list
    splitter: object
    docs: list
    embedding_model: object
    vectorstore: object
    retriever: object
    llm: object
    prompt: object

# =========================== NODES ===========================
from google.colab import userdata
# Load PDF
def load_pdf_node(state):
    loader = PyPDFLoader(state["pdf_path"])
    documents = loader.load_and_split()
    state["loader"] = loader
    state["documents"] = documents
    print(f"Loaded {len(documents)} pages")
    return state

# Split Text
def split_text_node(state):
    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = splitter.split_documents(state["documents"])
    state["splitter"] = splitter
    state["docs"] = docs
    print(f"Total chunks: {len(docs)}")
    return state

# Create Embeddings + Vectorstore
def embed_vectorstore_node(state):
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(state["docs"], embedding_model)
    retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k":5}, lambda_mult=0.3)
    state["embedding_model"] = embedding_model
    state["vectorstore"] = vectorstore
    state["retriever"] = retriever
    print("Embedding model and vectorstore created")
    return state

# Configure LLM + Prompt
def llm_prompt_node(state):
    llm = ChatOpenAI(model_name="gpt-4o-mini",
                     openai_api_key=os.environ['OPENAI_API_KEY'],
                     temperature=0.1,
                     max_tokens=1024)
    prompt = PromptTemplate.from_template(
        "Use the following context to answer the question:\n\n{context}\n\nQuestion: {question}"
    )
    state["llm"] = llm
    state["prompt"] = prompt
    print("LLM and Prompt configured")
    return state

# =========================== SETUP GRAPH ===========================
setup_graph = StateGraph(SetupChainState)

setup_graph.add_node("LoadPDF", load_pdf_node)
setup_graph.add_node("SplitText", split_text_node)
setup_graph.add_node("EmbedVectorstore", embed_vectorstore_node)
setup_graph.add_node("LLMConfig", llm_prompt_node)

setup_graph.set_entry_point("LoadPDF")
setup_graph.add_edge("LoadPDF", "SplitText")
setup_graph.add_edge("SplitText", "EmbedVectorstore")
setup_graph.add_edge("EmbedVectorstore", "LLMConfig")
setup_graph.add_edge("LLMConfig", END)

# =========================== VISUALIZE ===========================
chat_graph = setup_graph.compile()
display(Markdown("## Setup Chain Graph"))
from IPython.display import Image
Image(chat_graph.get_graph().draw_mermaid_png())

# =========================== RUN SETUP CHAIN ===========================
setup_app = setup_graph.compile()

pdf_path = "/content/solid-python.pdf"
setup_state = setup_app.invoke({"pdf_path": pdf_path})
print("\n Setup Chain Output State:")
for k in setup_state:
    print(f"{k}: {type(setup_state[k])}")



from langgraph.graph import MessagesState
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision, FactualCorrectness
import numpy as np

class RAGChainState(MessagesState):
    context: list[str]  # retrieved chunks
    answer: str         # final answer
    question: str       # user question
    ragas_data_input: dict # Data to create RAGAS dataset
    ragas_scores: dict      # Holds all 5 RAGAS metrics

# =========================== NODES ===========================

# Retrieve
def retrieve_node(state):
    retriever = setup_state["retriever"]
    relevant_docs = retriever.get_relevant_documents(state["question"])
    state["context"] = [doc.page_content for doc in relevant_docs]
    print(f"Retrieved {len(state['context'])} chunks")
    return state

# Prompt
def agumentation_prompt_node(state):
    prompt = setup_state["prompt"]
    formatted_prompt = prompt.format(context="\n\n".join(state["context"]), question=state["question"])
    state["messages"] = [formatted_prompt]
    print("Prompt formatted")
    return state

#SetDataset
def dataset_setup_node(state):
    # Prepare data for RAGAS dataset
    ground_truths = ["The main objective of the document is to explain solid python principles."]
    ragas_data_input = {
        "question": [state["question"]],
        "answer": [""],  # Placeholder for now
        "contexts": [state["context"]],
        "ground_truths": [ground_truths],
        "reference": [ground_truths[0]] # Add the 'reference' column
    }
    state["ragas_data_input"] = ragas_data_input
    print("RAGAS data input prepared")
    return state

#Evaluation
def ragas_evaluation_node(state):
    # Reconstruct RAGAS dataset from stored data input
    ragas_data = Dataset.from_dict(state["ragas_data_input"])

    # Update the answer column before evaluation
    ragas_data = ragas_data.map(lambda example: {"answer": state.get("answer", "")})

    results = evaluate(
        ragas_data,
        metrics=[
            context_precision, # Added () back
            context_recall,    # Added () back
            faithfulness,      # Added () back
            answer_relevancy,  # Added () back
            FactualCorrectness(), # Added () back
        ]
    )
    ragas_scores = results.to_pandas().to_dict(orient="records")[0]
    # Ensure all keys are strings for serialization
    # print(results)
    state["ragas_scores"] = {str(k): v for k, v in ragas_scores.items()}
    print(f"RAGAS Scores: {state['ragas_scores']}")
    return state

# LLM Invoke
def generation_llm_node(state):
    llm = setup_state["llm"]
    answer = llm.invoke(state["messages"])
    state["answer"] = answer.content
    print("Answer generated")
    return state

# =========================== RAG GRAPH ===========================
rag_graph = StateGraph(RAGChainState)

rag_graph.add_node("Retrieve", retrieve_node)
rag_graph.add_node("Agumentation", agumentation_prompt_node)
rag_graph.add_node("DatasetSetup", dataset_setup_node)
rag_graph.add_node("RAGASEvaluation", ragas_evaluation_node)
rag_graph.add_node("Generation", generation_llm_node)

# Edges
rag_graph.set_entry_point("Retrieve")
rag_graph.add_edge("Retrieve", "Agumentation")
rag_graph.add_edge("Agumentation", "DatasetSetup")
rag_graph.add_edge("DatasetSetup", "RAGASEvaluation")
rag_graph.add_edge("RAGASEvaluation", "Generation")
rag_graph.add_edge("Generation", END)

# =========================== VISUALIZE ===========================
app = rag_graph.compile()
display(Markdown("## Updated RAG Chain Graph with RAGAS Nodes"))
from IPython.display import Image
Image(app.get_graph().draw_mermaid_png())

# =========================== RUN RAG CHAIN ===========================
from langgraph.checkpoint.memory import InMemorySaver

memory = InMemorySaver()
app = rag_graph.compile(checkpointer=memory)

question_input = {"question": "What is the main objective of the document?"}
state = app.invoke(question_input, config={"configurable": {"thread_id": "some-thread-id"}})

print("\nFinal Answer:", state["answer"])
print("\nRAGAS Scores:\nContext_precision: ", state["ragas_scores"]['context_precision'])
print("Faithfulness: ",state["ragas_scores"]['faithfulness'])
print("Answer_relevancy: ",state["ragas_scores"]['answer_relevancy'])
print("Factual_correctness(mode=f1): ",state["ragas_scores"]['factual_correctness(mode=f1)'])
print("Context_recall: ",state["ragas_scores"]['context_recall'])

