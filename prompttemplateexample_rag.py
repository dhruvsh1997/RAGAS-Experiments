# -*- coding: utf-8 -*-
"""PromptTemplateExample-RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SH5bzrFo4Xs2sUHcXnuNQIvgo6Pt0bGY

## 1. `PromptTemplate`

### Description:
- `PromptTemplate` is the most basic template class.
- It is used to generate a string prompt by filling variables into a pre-defined template.
- Suitable for models that accept plain text as input (like OpenAI's `text-davinci` or `GPT-3` variants).

### Key Use-Cases:
- Prompting text-only models.
- Structuring basic question-answering prompts.
- Used in classical RAG pipelines where the prompt is just a single formatted string.
"""

# ===================== INSTALL DEPENDENCIES =====================
!pip install -q langchain sentence-transformers faiss-cpu pypdf groq langchain-community langchain-groq

# ===================== IMPORTS =====================
import os
import torch
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq
# from langchain_community.chat_models import ChatGroq # Corrected import path
from sentence_transformers.cross_encoder import CrossEncoder

import pandas as pd
from IPython.display import display, Markdown

# ===================== LOAD & SPLIT PDF =====================
loader = PyPDFLoader("/content/solid-python.pdf")
documents = loader.load_and_split()

splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)
print(f"Total Chunks Created: {len(docs)}")

# ===================== EMBEDDINGS + VECTORSTORE =====================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embedding_model)

# ===================== RETRIEVER WITH MMR =====================
retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5})
retriever

# ===================== DEFINE LLM =====================
from google.colab import userdata
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    api_key=userdata.get('GROQ_API_KEY')  # Replace with your Groq API key
)

# ===================== DEFINE PROMPT ===================
prompt_template = PromptTemplate.from_template(
    "Use the following context to answer the question:\n\n{context}\n\nQuestion: {question}"
)

# ===================== RERANKER INITIALIZATION =====================
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L6-v2")

# ===================== ASK A QUESTION =====================
question = "What is the main objective of the document?"

retrieved_docs = retriever.get_relevant_documents(question)

# Display retrieved chunks (before reranking)
print("\nðŸ”¹ Top K Retrieved Chunks (Before Reranking):")
for i, doc in enumerate(retrieved_docs):
    score = doc.metadata.get("score", "N/A")
    page = doc.metadata.get("page", "Unknown")
    print(f"\n--- Chunk {i+1} ---")
    print(f"Page: {page}")
    print(f"Content:\n{doc.page_content[:300]}...")

# ===================== RERANK CHUNKS =====================
pairs = [[question, doc.page_content] for doc in retrieved_docs]
scores = reranker.predict(pairs)

scored_docs = list(zip(retrieved_docs, scores))
sorted_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)

# Display chunks after reranking
print("\nðŸ”¸ Reranked Chunks (CrossEncoder):")
for i, (doc, score) in enumerate(sorted_docs):
    page = doc.metadata.get("page", "Unknown")
    print(f"\n--- Reranked Chunk {i+1} ---")
    print(f"Page: {page}")
    print(f"Rerank Score: {score:.4f}")
    print(f"Content:\n{doc.page_content[:300]}...")

# ===================== FINAL ANSWERS =====================

# Step 1: Answer using pre-reranked chunks
context_before = "\n\n".join([doc.page_content for doc in retrieved_docs[:3]])
prompt_before = prompt_template.format(context=context_before, question=question)
answer_before = llm.invoke(prompt_before)

# Step 2: Answer using top reranked chunks
top_reranked_docs = [doc for doc, _ in sorted_docs[:3]]
context_after = "\n\n".join([doc.page_content for doc in top_reranked_docs])
prompt_after = prompt_template.format(context=context_after, question=question)
answer_after = llm.invoke(prompt_after)

# ===================== DISPLAY RESULTS =====================
display(Markdown("### Final Answer (Before Reranking):"))
display(Markdown(answer_before.content)) # Extract the text content

display(Markdown("### Final Answer (After Reranking):"))
display(Markdown(answer_after.content)) # Extract the text content

