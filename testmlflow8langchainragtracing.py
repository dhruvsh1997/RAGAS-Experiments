# -*- coding: utf-8 -*-
"""TestMLFlow8LangchainRAGTracing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H1ZYnmy3RtwX7KNbjP_Fet06M_Cb2zcY
"""

# Cell 1: Install required packages
!pip install langchain langchain-community langchain-core python-dotenv faiss-cpu sentence-transformers pypdf mlflow langchain-groq detoxify

# Cell 2: Import necessary libraries
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_groq import ChatGroq
from langchain.chains import RetrievalQA
from langchain_core.runnables import chain
from langchain_core.caches import BaseCache
from langchain_core import callbacks
from langchain.cache import InMemoryCache
import mlflow
from pydantic import BaseModel, Field
import numpy as np
from sentence_transformers import SentenceTransformer

# Cell 3: Configure environment variables
from google.colab import userdata
os.environ["GROQ_API_KEY"] = userdata.get('GROQ_API_KEY')  # Replace with your key
# os.environ["MLFLOW_TRACKING_URI"] = "http://localhost:5000"  # Replace with your MLFlow tracking URI

# Initialize MLFlow
mlflow.set_experiment("rag-chatbot-evaluation")

# Cell 4: Define PDF path and load document
PDF_PATH = "/content/testPDF.pdf"

@mlflow.trace(name="load_pdf", span_type="RETRIEVER")
def load_pdf(pdf_path):
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()
    mlflow.log_param("pdf_path", pdf_path)
    mlflow.log_metric("num_pages_loaded", len(documents))
    return documents

# Load the documents
documents = load_pdf(PDF_PATH)
print(f"Number of pages loaded: {len(documents)}")

# Cell 5: Split documents into chunks
@mlflow.trace(name="split_documents", span_type="PARSER")
def split_documents(documents):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    mlflow.log_metric("num_chunks_created", len(chunks))
    return chunks

chunks = split_documents(documents)
print(f"Number of chunks created: {len(chunks)}")
# Inspect first chunk
print("\nFirst chunk content:")
print(chunks[0].page_content[:200])  # First 200 characters

# Cell 6: Create embeddings and vector store
@mlflow.trace(name="create_vectorstore", span_type="EMBEDDING")
def create_vectorstore(chunks):
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(chunks, embeddings)
    mlflow.log_param("embedding_model", "all-MiniLM-L6-v2")
    return vectorstore

vectorstore = create_vectorstore(chunks)
print("Vector store created successfully")

# Test similarity search
test_search = vectorstore.similarity_search("test query", k=1)
print("\nSample similarity search result:")
print(test_search[0].page_content[:200])

# Cell 7: Set up the LLM and prompt
@mlflow.trace(name="setup_llm", span_type="LLM")
def setup_llm():
    llm = ChatGroq(
        model_name="llama-3.3-70b-versatile",
        temperature=0.3,
        groq_api_key=os.environ["GROQ_API_KEY"]
    )
    mlflow.log_param("llm_model", "llama-3.3-70b-versatile")
    mlflow.log_param("temperature", 0.3)
    return llm

llm = setup_llm()
llm

# Create prompt template
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

{context}

Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template=template,
)

# Cell 8: Create RAG chain
@mlflow.trace(name="create_rag_chain", span_type="CHAIN")
def create_rag_chain(llm, vectorstore, prompt):
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )
    mlflow.log_param("retriever_search_kwargs", {"k": 3})
    return qa_chain

qa_chain = create_rag_chain(llm, vectorstore, QA_CHAIN_PROMPT)
qa_chain

# Cell 9: Test the chain with a sample question
test_question = "What is GIT?"
response = qa_chain({"query": test_question})
print("Sample question:", test_question)
print("\nAnswer:", response["result"])

print("\nSource Documents Used:")
for i, doc in enumerate(response["source_documents"], 1):
    print(f"\nDocument {i}:")
    print(doc.page_content[:200])

# Cell 10: Function to trigger RAG QA chain and store questions and responses
@mlflow.trace(name="trigger_rag_qa_chain", span_type="CHAIN")
def trigger_rag_qa_chain(question):
    response = qa_chain({"query": question})
    # Replacing invalid characters with underscores:
    valid_question_name = f"question_{question.replace('?', '').replace(' ', '_')}"
    mlflow.log_param(valid_question_name, response["result"])
    return response
# Example usage1:
question = "Explain Features of GIT?"
response = trigger_rag_qa_chain(question)

from detoxify import Detoxify
# Cell 11: Function to perform evaluation based on saved dataset
@mlflow.trace(name="perform_evaluation", span_type="CHAIN")
def perform_evaluation(dataset):
    if not dataset:
        print("Dataset is empty. No evaluation to perform.")
        return

    # Initialize the sentence transformer model
    model = SentenceTransformer('all-MiniLM-L6-v2')
    # Initialize the toxicity model
    toxicity_model = Detoxify('original')

    # Initialize lists to store similarity scores, toxicity scores, and latency
    similarity_scores = []
    toxicity_scores = []
    latency_scores = []

    for question, ground_truth in dataset.items():
        response = trigger_rag_qa_chain(question)
        response_text = response["result"]

        # Calculate similarity score
        response_vector = model.encode(response_text)
        ground_truth_vector = model.encode(ground_truth)
        similarity_score = np.dot(response_vector, ground_truth_vector) / (np.linalg.norm(response_vector) * np.linalg.norm(ground_truth_vector))
        similarity_scores.append(similarity_score)

        # Calculate toxicity score using Detoxify
        toxicity_results = toxicity_model.predict(response_text)
        # Assuming you want the overall toxicity score
        toxicity_score = toxicity_results['toxicity']
        toxicity_scores.append(toxicity_score)

        # Calculate latency - This likely needs to be implemented using timers
        # For now, it's a placeholder
        # latency_score = mlflow.metrics.latency(response_text) # This doesn't exist
        latency_score = 0  # Placeholder
        latency_scores.append(latency_score)

    # Log evaluation results to MLFlow
    with mlflow.start_run(nested=True):
        mlflow.log_metric("average_similarity_score", np.mean(similarity_scores))
        mlflow.log_metric("average_toxicity_score", np.mean(toxicity_scores))
        mlflow.log_metric("average_latency_score", np.mean(latency_scores))

# Example dataset
dataset = {
    "What is GIT?": "GIT is a version control system.",
    "Explain Features of GIT?": "GIT features include distributed version control, data integrity, and support for non-linear workflows."
}

# Perform evaluation
perform_evaluation(dataset)

!pip install pyngrok

import os
import getpass
from pyngrok import ngrok, conf
from google.colab import userdata
os.environ["NGROK"]=userdata.get('NGROK_TOKEN')

conf.get_default().auth_token = os.environ["NGROK"]

# Set up ngrok tunnel for MLflow UI
ngrok.kill()
ngrok_tunnel = ngrok.connect(addr="5000", proto="http", bind_tls=True)
print("MLflow Tracking UI:", ngrok_tunnel.public_url)

!mlflow ui









"""Act as a Python, Langchain, Langsmith and LangGraph Expert who have lots of experiance in working with Agentic AI systems and Langchain 0.3. I have this code that is performing Langchain RAG with Langsmith where its using most of the langchain code from langchain 0.1, Now I need your help to rewrite this code with change all the tracking from langsmith to MLFlow, basically everything that is langsmith doing right now will be done by mlflow in the new code, from monitering pdf extraction,text splitting, embadding, vactorstore creation, (GROQ) llmchain creation, question answering, prompt, question_set(dataset), evaluation and so on, all will be now managed by MLFlow. Here is my RAG Code, remember I want you to rewrite this exact code basicallyeverything else will be the same list langsmith will be replaced with MLFlow :
"""# Cell 1: Install required packages
!pip install langchain langchain-community langchain-core python-dotenv faiss-cpu sentence-transformers pypdf langsmith langchain-groq
# Cell 2: Import necessary libraries
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_groq import ChatGroq
from langchain.chains import RetrievalQA
from langchain.smith import RunEvalConfig, run_on_dataset
from langsmith import Client, traceable
from langchain_core.runnables import chain
from langchain.smith.evaluation.config import RunEvalConfig
from langchain_groq import ChatGroq
from langchain_core.caches import BaseCache
from langchain_core import callbacks
from langchain.cache import InMemoryCache
# Cell 3: Configure environment variables
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "key"  # Replace with your key
os.environ["LANGCHAIN_ENDPOINT"]="https://api.smith.langchain.com"
os.environ["LANGCHAIN_PROJECT"] = "rag-chatbot-evaluation"
os.environ["GROQ_API_KEY"] = "key"  # Replace with your key\

# Initialize LangSmith client
client = Client()
# Cell 4: Define PDF path and load document
PDF_PATH = "/content/testPDF.pdf"

@traceable(name="load_pdf")
def load_pdf(pdf_path):
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()
    return documents

# Load the documents
documents = load_pdf(PDF_PATH)
print(f"Number of pages loaded: {len(documents)}")

# Cell 5: Split documents into chunks
@traceable(name="split_documents")
def split_documents(documents):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    return chunks

chunks = split_documents(documents)
print(f"Number of chunks created: {len(chunks)}")
# Inspect first chunk
print("\nFirst chunk content:")
print(chunks[0].page_content[:200])  # First 200 characters

# Cell 6: Create embeddings and vector store
@traceable(name="create_vectorstore")
def create_vectorstore(chunks):
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(chunks, embeddings)
    return vectorstore

vectorstore = create_vectorstore(chunks)
print("Vector store created successfully")

# Test similarity search
test_search = vectorstore.similarity_search("test query", k=1)
print("\nSample similarity search result:")
print(test_search[0].page_content[:200])

# Cell 7: Set up the LLM and prompt
@traceable(name="setup_llm")
def setup_llm():
    return ChatGroq(
        model_name="llama-3.3-70b-versatile",
        temperature=0.3,
        groq_api_key=os.environ["GROQ_API_KEY"]
    )

llm = setup_llm()

# fetching prompt from langsmith profile
from langchain import hub
prompt = hub.pull("rag-chatbot-evaluation-sys-p1/rag-chatbot-evaluation-sys-p1")
prompt

# Create prompt template
template = dict(dict(prompt[0])['prompt'])['template']

QA_CHAIN_PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template=template,
)
QA_CHAIN_PROMPT

# Cell 8: Create RAG chain
@traceable(name="create_rag_chain")
def create_rag_chain(llm, vectorstore, prompt):
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )
    return qa_chain

qa_chain = create_rag_chain(llm, vectorstore, QA_CHAIN_PROMPT)
qa_chain

# Cell 9: Test the chain with a sample question
test_question = "What is GIT?"
response = qa_chain({"query": test_question})
print("Sample question:", test_question)
print("\nAnswer:", response["result"])

print("\nSource Documents Used:")
for i, doc in enumerate(response["source_documents"], 1):
    print(f"\nDocument {i}:")
    print(doc.page_content[:200])
"""

In this I want to moniter every single process in this through mlflow automate traceing, If you need some documentation, here is some small code snippits example how to track Langchain through MLFLow

Example 1
"
pip install mlflow==2.18.0 langchain==0.3.0 langchain-openai==0.2.9

import mlflow
import os

from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

mlflow.set_experiment("LangChain Tracing")

# Enabling autolog for LangChain will enable trace logging.
mlflow.langchain.autolog()

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7, max_tokens=1000)

prompt_template = PromptTemplate.from_template(
    "Answer the question as if you are {person}, fully embodying their style, wit, personality, and habits of speech. "
    "Emulate their quirks and mannerisms to the best of your ability, embracing their traits—even if they aren't entirely "
    "constructive or inoffensive. The question is: {question}"
)

chain = prompt_template | llm | StrOutputParser()

# Let's test another call
chain.invoke(
    {
        "person": "Linus Torvalds",
        "question": "Can I just set everyone's access to sudo to make things easier?",
    }
)
"
Example 2
"
import mlflow

# Create a new experiment to log the trace to
mlflow.set_experiment("Tracing Demo")


# Mark any function with the trace decorator to automatically capture input(s) and output(s)
@mlflow.trace
def some_function(x, y, z=2):
    return x + (y - z)


# Invoking the function will generate a trace that is logged to the active experiment
some_function(2, 4)

@mlflow.trace(name="My Span", span_type="func", attributes={"a": 1, "b": 2})
def my_func(x, y):
    return x + y
"

Example 3
"
from openai import AsyncOpenAI

client = AsyncOpenAI()


@mlflow.trace
async def async_func(message: str):
    return await client.chat.completion.create(
        model="gpt-4o", messages=[{"role": "user", "content": message}]
    )


await async_func("What is MLflow Tracing?")
"
Example 4
"
import mlflow


@mlflow.trace(span_type="func", attributes={"key": "value"})
def add_1(x):
    return x + 1


@mlflow.trace(span_type="func", attributes={"key1": "value1"})
def minus_1(x):
    return x - 1


@mlflow.trace(name="Trace Test")
def trace_test(x):
    step1 = add_1(x)
    return minus_1(step1)


trace_test(4)
"
Example 5:
"
import mlflow
from mlflow.entities import SpanType


# Using a built-in span type
@mlflow.trace(span_type=SpanType.RETRIEVER)
def retrieve_documents(query: str):
    ...


# Setting a custom span type
with mlflow.start_span(name="add", span_type="MATH") as span:
    span.set_inputs({"x": z, "y": y})
    z = x + y
    span.set_outputs({"z": z})

    print(span.span_type)
    # Output: MATH
"
Example 7:
"import mlflow


@mlflow.trace
def first_func(x, y=2):
    return x + y


@mlflow.trace
def second_func(a, b=3):
    return a * b


def do_math(a, x, operation="add"):
    # Use the fluent API context handler to create a new span
    with mlflow.start_span(name="Math") as span:
        # Specify the inputs and attributes that will be associated with the span
        span.set_inputs({"a": a, "x": x})
        span.set_attributes({"mode": operation})

        # Both of these functions are decorated for tracing and will be associated
        # as 'children' of the parent 'span' defined with the context handler
        first = first_func(x)
        second = second_func(a)

        result = None

        if operation == "add":
            result = first + second
        elif operation == "subtract":
            result = first - second
        else:
            raise ValueError(f"Unsupported Operation Mode: {operation}")

        # Specify the output result to the span
        span.set_outputs({"result": result})

        return result
"
Example 8:
"# Cell 10: Run evaluation
from pydantic import BaseModel, Field
import numpy as np
from sentence_transformers import SentenceTransformer
# Define the application logic you want to evaluate inside a target function
def target(inputs):
    response = qa_chain({"query": inputs})  # Access qa_chain from the closure
    return {"response": response["result"].strip()}

instructions = """Evaluate Document Related Answer against Ground Truth for conceptual similarity and classify true or false:
- False: No conceptual match and similarity
- True: Most or full conceptual match and similarity
- Key criteria: Concept should match, not exact wording.
"""
# Define LLM judge that grades the context similarity score of the response relative to reference output
def average_similarity_score(outputs, reference_outputs):
    # Initialize the list to store similarity scores
    similarity_scores = []

    # Initialize the sentence transformer model
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # Iterate through the outputs and reference outputs
    for output, reference_output in zip(outputs, reference_outputs):
        # Get the response and reference output
        # breakpoint()
        response = target(output)["response"]
        reference_output_text = reference_output

        # Encode the response and reference output into vectors
        response_vector = model.encode(response)
        reference_output_vector = model.encode(reference_output_text)

        # Calculate the cosine similarity between the two vectors
        similarity_score = np.dot(response_vector, reference_output_vector) / (np.linalg.norm(response_vector) * np.linalg.norm(reference_output_vector))

        # Append the similarity score to the list
        similarity_scores.append(similarity_score)

    # Calculate the average similarity score
    average_score = np.mean(similarity_scores)

    return average_score
# Log evaluation results to MLFlow
with mlflow.start_run(nested=True):
    mlflow.log_param("evaluation_instructions", instructions)
    mlflow.log_param("evaluation_questions", evl_data['query'])
    mlflow.log_metric("accuracy_score", average_similarity_score(evl_data['query'], evl_data['ground_truth']))
def create_evaluation_dataset(chain: chain, questions: list):
    dataset_name = "rag-qa-eval"
    with mlflow.start_run(nested=True):
        for i, question in enumerate(questions): # using enumerate for question index
            result = chain({"query": question})
            # Replacing invalid characters with underscores:
            valid_question_name = f"question_{i + 1}_{question.replace('?', '').replace(' ', '_')}"
            mlflow.log_param(valid_question_name, result)
        return dataset_name

dataset_name = create_evaluation_dataset(qa_chain, evl_data['query'])
print(f"Created evaluation dataset: {dataset_name}")"

analyse all 8 examples and rewrite my code properly according to the requirements.
Lastly I want you to create a full fledge code in an interprative manner just like its already written like I am writing this on Google Colab.

Existing Span types in trace Mlflow """Span Type

Description

"LLM"

Represents a call to an LLM endpoint or a local model.

"CHAT_MODEL"

Represents a query to a chat model. This is a special case of an LLM interaction.

"CHAIN"

Represents a chain of operations.

"AGENT"

Represents an autonomous agent operation.

"TOOL"

Represents a tool execution (typically by an agent), such as querying a search engine.

"EMBEDDING"

Represents a text embedding operation.

"RETRIEVER"

Represents a context retrieval operation, such as querying a vector database.

"PARSER"

Represents a parsing operation, transforming text into a structured format.

"RERANKER"

Represents a re-ranking operation, ordering the retrieved contexts based on relevance.

"UNKNOWN"

A default span type that is used when no other span type is specified."""

In the end make sure to create a function which will trigger rag qachain in teh end and whatever question we are asking is being stored in a dictionary along with the response and save that response into dataset mlflow, and create another function which will perform evaluation based on this saved dataset if its not empty where saved question and answer will act as query and ground truth and the llm response on those query in the evaluation will be the response that will be compared to ground truth, and then check anc calculate "mlflow.metrics.toxicity(), mlflow.metrics.latency(), mlflow.metrics.genai.answer_similarity()"in mlflow.
"""