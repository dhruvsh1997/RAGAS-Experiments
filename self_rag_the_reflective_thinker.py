# -*- coding: utf-8 -*-
"""Self_RAG_The_Reflective_Thinker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nFsEaRznkttxev5ngX8OF1IrwXV6qFsO
"""

# Install LangChain 0.3, FAISS, and sentence-transformers
!pip install -U langchain langchain-community langchain_groq
!pip install faiss-cpu
!pip install sentence-transformers pypdf
!pip install groq

# Mount Google Drive so we can access KnowledgePDF folder
from google.colab import drive
drive.mount('/content/drive')

from langchain_community.document_loaders import PyPDFLoader
import os

# Define path to your PDF folder
pdf_folder = "/content/drive/MyDrive/Colab Notebooks/AdvanceRAG Problems/dataPDF"

# Load all PDFs in the folder
loaders = []
for filename in os.listdir(pdf_folder):
    if filename.endswith(".pdf"):
        loaders.append(PyPDFLoader(os.path.join(pdf_folder, filename)))

# Combine all documents
documents = []
for loader in loaders:
    documents.extend(loader.load())

print(f"Loaded {len(documents)} PDF pages.")

from langchain_text_splitters import RecursiveCharacterTextSplitter

# Configure your splitter for meaningful, overlapping chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

chunks = text_splitter.split_documents(documents)
print(f"Total chunks created: {len(chunks)}")

from langchain_community.embeddings import HuggingFaceEmbeddings

# Use HuggingFaceEmbeddings with sentence-transformers
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-V2"
)

# Embed and store in FAISS vector DB
from langchain_community.vectorstores import FAISS

vectorstore = FAISS.from_documents(chunks, embedding_model)

# Save index if needed
vectorstore.save_local("faiss_index_epic_story")

# Configure Retriever for Maximum Marginal Relevance
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"lambda_mult": 0.5, "k": 5}
)

from langchain_groq import ChatGroq
from google.colab import userdata
# Connect to Groq with your API key
groq_api_key = userdata.get('GROQ_API_KEY')

# First LLM for generation
rag_llm = ChatGroq(
    api_key=groq_api_key,
    model_name="gemma2-9b-it"
)

# Second LLM for final editing
editor_llm = ChatGroq(
    api_key=groq_api_key,
    model_name="llama-3.3-70b-versatile"
)

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

# Define your prompt for the RAG generation
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are an Technical Network Engineer. Use retrieved context to write the answer related to my query or any hypothatical schenero thats look similar to conext Info."),
    ("user", "{context}\n\nQuestion prompt: {question}")
])

# Define your prompt for the Editor step
editor_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a master network tester and editor. Refine the answer to ensure consistency and accuracy."),
    ("user", "{draft}")
])

# JSON parser for output parsing if needed
output_parser = JsonOutputParser()

from langchain_community.vectorstores import FAISS

# Create another FAISS for internal chapter storage
internal_store = FAISS.from_texts(["dummy"], embedding_model)

## This will hold your generated chapters as they grow
def self_rag_retrieve(query):
    # Search in internal outputs first
    internal_results = internal_store.similarity_search(query)
    if len(internal_results) < 1:  # Adjust sufficiency threshold as needed
        external_results = retriever.get_relevant_documents(query)
        return external_results
    return internal_results

from langchain_core.runnables import RunnablePassthrough, RunnableSequence

# Self-RAG: Retrieve -> Generate draft -> Edit -> Store
def generate_answer(query_prompt):
    # Step 1: Retrieve
    context_docs = self_rag_retrieve(query_prompt)
    context = "\n\n".join([doc.page_content for doc in context_docs])

    # Step 2: Generate draft with first LLM
    draft = rag_llm.invoke(
        rag_prompt.format(context=context, question=query_prompt)
    )

    # Step 3: Refine with editor LLM
    final = editor_llm.invoke(
        editor_prompt.format(draft=draft.content)
    )

    # Step 4: Store final Query in internal store
    internal_store.add_texts([final.content])

    return final.content

# Test with your first chapter prompt
first_chapter = generate_answer("What is OSI layer and how it works?")
print(first_chapter)

# You can run this multiple times for each new Details
next_prompt = "How its different from TCP/IP."
second_chapter = generate_answer(next_prompt)
print(second_chapter)



